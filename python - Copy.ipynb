{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched URL: blackassign0001\n",
      "Successfully fetched URL: blackassign0002\n",
      "Successfully fetched URL: blackassign0003\n",
      "Successfully fetched URL: blackassign0004\n",
      "Successfully fetched URL: blackassign0005\n",
      "Successfully fetched URL: blackassign0006\n",
      "Successfully fetched URL: blackassign0007\n",
      "Successfully fetched URL: blackassign0008\n",
      "Successfully fetched URL: blackassign0009\n",
      "Successfully fetched URL: blackassign0010\n",
      "Successfully fetched URL: blackassign0011\n",
      "Successfully fetched URL: blackassign0012\n",
      "Successfully fetched URL: blackassign0013\n",
      "Successfully fetched URL: blackassign0014\n",
      "Successfully fetched URL: blackassign0015\n",
      "Successfully fetched URL: blackassign0016\n",
      "Successfully fetched URL: blackassign0017\n",
      "Successfully fetched URL: blackassign0018\n",
      "Successfully fetched URL: blackassign0019\n",
      "Successfully fetched URL: blackassign0020\n",
      "Successfully fetched URL: blackassign0021\n",
      "Successfully fetched URL: blackassign0022\n",
      "Successfully fetched URL: blackassign0023\n",
      "Successfully fetched URL: blackassign0024\n",
      "Successfully fetched URL: blackassign0025\n",
      "Successfully fetched URL: blackassign0026\n",
      "Successfully fetched URL: blackassign0027\n",
      "Successfully fetched URL: blackassign0028\n",
      "Successfully fetched URL: blackassign0029\n",
      "Successfully fetched URL: blackassign0030\n",
      "Successfully fetched URL: blackassign0031\n",
      "Successfully fetched URL: blackassign0032\n",
      "Successfully fetched URL: blackassign0033\n",
      "Successfully fetched URL: blackassign0034\n",
      "Successfully fetched URL: blackassign0035\n",
      "Failed to fetch URL: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Successfully fetched URL: blackassign0037\n",
      "Successfully fetched URL: blackassign0038\n",
      "Successfully fetched URL: blackassign0039\n",
      "Successfully fetched URL: blackassign0040\n",
      "Successfully fetched URL: blackassign0041\n",
      "Successfully fetched URL: blackassign0042\n",
      "Successfully fetched URL: blackassign0043\n",
      "Successfully fetched URL: blackassign0044\n",
      "Successfully fetched URL: blackassign0045\n",
      "Successfully fetched URL: blackassign0046\n",
      "Successfully fetched URL: blackassign0047\n",
      "Successfully fetched URL: blackassign0048\n",
      "Failed to fetch URL: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Successfully fetched URL: blackassign0050\n",
      "Successfully fetched URL: blackassign0051\n",
      "Successfully fetched URL: blackassign0052\n",
      "Successfully fetched URL: blackassign0053\n",
      "Successfully fetched URL: blackassign0054\n",
      "Successfully fetched URL: blackassign0055\n",
      "Successfully fetched URL: blackassign0056\n",
      "Successfully fetched URL: blackassign0057\n",
      "Successfully fetched URL: blackassign0058\n",
      "Successfully fetched URL: blackassign0059\n",
      "Successfully fetched URL: blackassign0060\n",
      "Successfully fetched URL: blackassign0061\n",
      "Successfully fetched URL: blackassign0062\n",
      "Successfully fetched URL: blackassign0063\n",
      "Successfully fetched URL: blackassign0064\n",
      "Successfully fetched URL: blackassign0065\n",
      "Successfully fetched URL: blackassign0066\n",
      "Successfully fetched URL: blackassign0067\n",
      "Successfully fetched URL: blackassign0068\n",
      "Successfully fetched URL: blackassign0069\n",
      "Successfully fetched URL: blackassign0070\n",
      "Successfully fetched URL: blackassign0071\n",
      "Successfully fetched URL: blackassign0072\n",
      "Successfully fetched URL: blackassign0073\n",
      "Successfully fetched URL: blackassign0074\n",
      "Successfully fetched URL: blackassign0075\n",
      "Successfully fetched URL: blackassign0076\n",
      "Successfully fetched URL: blackassign0077\n",
      "Successfully fetched URL: blackassign0078\n",
      "Successfully fetched URL: blackassign0079\n",
      "Successfully fetched URL: blackassign0080\n",
      "Successfully fetched URL: blackassign0081\n",
      "Successfully fetched URL: blackassign0082\n",
      "Successfully fetched URL: blackassign0083\n",
      "Successfully fetched URL: blackassign0084\n",
      "Successfully fetched URL: blackassign0085\n",
      "Successfully fetched URL: blackassign0086\n",
      "Successfully fetched URL: blackassign0087\n",
      "Successfully fetched URL: blackassign0088\n",
      "Successfully fetched URL: blackassign0089\n",
      "Successfully fetched URL: blackassign0090\n",
      "Successfully fetched URL: blackassign0091\n",
      "Successfully fetched URL: blackassign0092\n",
      "Successfully fetched URL: blackassign0093\n",
      "Successfully fetched URL: blackassign0094\n",
      "Successfully fetched URL: blackassign0095\n",
      "Successfully fetched URL: blackassign0096\n",
      "Successfully fetched URL: blackassign0097\n",
      "Successfully fetched URL: blackassign0098\n",
      "Successfully fetched URL: blackassign0099\n",
      "Successfully fetched URL: blackassign0100\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "extracted_data = []\n",
    "\n",
    "for index, row in url_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_title = soup.find('h1').text.strip()\n",
    "        article_text = soup.find('div', class_='td-post-content')\n",
    "        \n",
    "        for tag in article_text.find_all('pre', class_='wp-block-preformatted'):\n",
    "            tag.decompose()\n",
    "        \n",
    "        article_text = article_text.text.strip()\n",
    "        \n",
    "        extracted_data.append({'URL_ID': url_id, 'Title': article_title, 'Text': article_text})\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Failed to fetch URL: {url}\")\n",
    "\n",
    "extracted_data_df = pd.DataFrame(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>Rising IT cities and its impact on the economy...</td>\n",
       "      <td>We have seen a huge development and dependence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>Rising IT Cities and Their Impact on the Econo...</td>\n",
       "      <td>Throughout history, from the industrial revolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>Internet Demand’s Evolution, Communication Imp...</td>\n",
       "      <td>Introduction\\nIn the span of just a few decade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>Rise of Cybercrime and its Effect in upcoming ...</td>\n",
       "      <td>The way we live, work, and communicate has unq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>OTT platform and its impact on the entertainme...</td>\n",
       "      <td>The year 2040 is poised to witness a continued...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                              Title  \\\n",
       "0  blackassign0001  Rising IT cities and its impact on the economy...   \n",
       "1  blackassign0002  Rising IT Cities and Their Impact on the Econo...   \n",
       "2  blackassign0003  Internet Demand’s Evolution, Communication Imp...   \n",
       "3  blackassign0004  Rise of Cybercrime and its Effect in upcoming ...   \n",
       "4  blackassign0005  OTT platform and its impact on the entertainme...   \n",
       "\n",
       "                                                Text  \n",
       "0  We have seen a huge development and dependence...  \n",
       "1  Throughout history, from the industrial revolu...  \n",
       "2  Introduction\\nIn the span of just a few decade...  \n",
       "3  The way we live, work, and communicate has unq...  \n",
       "4  The year 2040 is poised to witness a continued...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the folder containing stop words text files\n",
    "stopwords_folder_path = r'C:\\Users\\LENOVO\\Downloads\\BlackCoffer\\StopWords-20240507T120351Z-001\\StopWords'\n",
    "\n",
    "# Function to extract stop words from a text file\n",
    "def extract_stopwords(file_path):\n",
    "    stopwords = set()  # Use a set to store unique stop words\n",
    "    encodings = ['utf-8', 'latin1', 'windows-1252']  # Add additional encodings as needed\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                for line in file:\n",
    "                    # Split the line based on the pipe character (|)\n",
    "                    stop_word = line.split('|')[0].strip()  # Extract the stop word and remove leading/trailing spaces\n",
    "                    stopwords.add(stop_word)\n",
    "            return stopwords\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    # If all encodings fail\n",
    "    raise Exception(\"Unable to decode file using any of the specified encodings\")\n",
    "\n",
    "# Initialize a dictionary to store stop words from each file\n",
    "stopwords_dict = {}\n",
    "\n",
    "# Iterate over each text file in the folder\n",
    "for file_name in os.listdir(stopwords_folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(stopwords_folder_path, file_name)\n",
    "        stopwords = extract_stopwords(file_path)\n",
    "        stopwords_dict[file_name] = stopwords\n",
    "\n",
    "# Now stopwords_dict will contain stop words extracted from each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all words in the 'Title' and 'Text' columns to lowercase\n",
    "extracted_data_df['Title'] = extracted_data_df['Title'].str.lower()\n",
    "extracted_data_df['Text'] = extracted_data_df['Text'].str.lower()\n",
    "\n",
    "# Convert all stopwords in the stopwords dictionary to lowercase\n",
    "for file_name, stopwords in stopwords_dict.items():\n",
    "    stopwords_dict[file_name] = {word.lower() for word in stopwords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords from text using the dictionary of stopwords\n",
    "def remove_stopwords(text, stopwords_dict):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    # Initialize an empty list to store non-stopwords\n",
    "    filtered_words = []\n",
    "    # Iterate over each word in the text\n",
    "    for word in words:\n",
    "        # Check if the word is not in any of the stopwords sets in the stopwords_dict\n",
    "        if not any(word in stopwords_set for stopwords_set in stopwords_dict.values()):\n",
    "            # If the word is not a stopword, add it to the filtered list\n",
    "            filtered_words.append(word)\n",
    "    # Join the filtered words back into a single string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "# Remove stopwords from the 'Title' column\n",
    "extracted_data_df['Title'] = extracted_data_df['Title'].apply(lambda x: remove_stopwords(x, stopwords_dict))\n",
    "\n",
    "# Remove stopwords from the 'Text' column\n",
    "extracted_data_df['Text'] = extracted_data_df['Text'].apply(lambda x: remove_stopwords(x, stopwords_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huge development dependence people technology recent years. development chatgpt recent years. normal thing fully dependent technology 2040. information technology developing nations. member developing nation, rapidly growing base. grown cities control centres information technology 2040. rising cities noida:- noida uttar pradesh emerging sector now. companies google, microsoft, ibm, infosys set companies here. noida market base billions dollars great job boosting national economy. establishment software companies made noida information technology hub. gurgaon:- gurgaon haryana emerging hub. companies google, microsoft, ibm, infosys set companies here. gurgaon market base billions dollars great job boosting national economy. bengaluru:- bengaluru called hub india. city. companies google, microsoft, ibm, infosys set companies here. bengaluru market base billions dollars great job boosting national economy. kolkata:- kolkata bengal emerging hub. kolkata i.e. saltlake sector 5, town, rajarhat area kolkata hub. government giving software companies cost set companies there. companies google, microsoft, ibm, infosys set companies here. kolkata market base billions dollars great job boosting national economy. impact economy huge impact rising cities economy. effects are- demand:- rising cities greatly boost economy. create huge demand raw materials. products huge demand people too. supply:-– supply fulfilment demand. highly populous india, demand finished products. cities develop, companies fulfil desires people populous india. cities develop, companies come, supply finished products people. market: market economic agents buyers interact another. populous india, huge market. cities grow, companies world competition market increase. consumers differentiated products market run smoothly. competitive market healthy. safely assume oligopoly market surely tend reach perfectly competitive market 2040. revenue:- market increases, revenue generated. present, revenue 245 dollars, 19 dollars financial 2022. cities grow, companies invest leads increase market turn generates revenue india. expect revenue tend reach 10 dollars 2040. impact environment rising cities create huge impact environment, maximum harmful effects. impact rising cities environment is- deforestation:- cutting trees huge make building companies great harm environment. cutting trees scale mass degradation forests. carbon footprint:- companies generate carbon footprint atmosphere. asian countries including carbon footprint. sector grows generation carbon footprint 2040. death birds:- cell phone mobile towers telecom companies caused death birds caused great imbalance ecosystem. number sparrows reduced due phenomenon. extinction species 2040. impact infrastructure contributions cities infrastructure. are- transportation:- rising cities excellent transport system supply raw materials delivery finished products market. transportation system develops area. excellent transport system 2040. public transport system:- public transport system cities. cities source employment huge population reside areas, adequate public transport systems buses, taxis etc. improved 2040. water supply:- huge number people reside cities adequate water supply fulfil people industries. find methods water supply conservation 2040. electricity:- electric supply lifeline sector. electric supply, machines run cities flourish. cities flourish way, excellent electric supply 2040. healthcare:- number people reside cities, proper health infrastructure healthcare facilities people. growth cities, healthcare system improve 2040. education:- education primary nation. proper education training centres cities fulfil people’s demands. growth cities, education system develop 2040. education skill-oriented. impact life growth cities, people jobs earn more. purchasing people increase. people lead lifestyle. buy things value. tastes preferences people change. human development index increase. people buy quality food quality cars. food, automobile industries increase. huge impact life 2040.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data_df['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the folder containing stop words text files\n",
    "stopwords_folder_path = r'C:\\Users\\LENOVO\\Downloads\\BlackCoffer\\MasterDictionary-20240507T120347Z-001\\MasterDictionary'\n",
    "\n",
    "# Function to extract stop words from a text file\n",
    "def extract_stopwords(file_path):\n",
    "    stopwords = set()  # Use a set to store unique stop words\n",
    "    encodings = ['utf-8', 'latin1', 'windows-1252']  # Add additional encodings as needed\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                for line in file:\n",
    "                    # Split the line based on the pipe character (|)\n",
    "                    stop_word = line.split('|')[0].strip()  # Extract the stop word and remove leading/trailing spaces\n",
    "                    stopwords.add(stop_word)\n",
    "            return stopwords\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    # If all encodings fail\n",
    "    raise Exception(\"Unable to decode file using any of the specified encodings\")\n",
    "\n",
    "# Initialize a dictionary to store stop words from each file\n",
    "positive_and_negative_words_dict = {}\n",
    "\n",
    "# Iterate over each text file in the folder\n",
    "for file_name in os.listdir(stopwords_folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(stopwords_folder_path, file_name)\n",
    "        stopwords = extract_stopwords(file_path)\n",
    "        positive_and_negative_words_dict[file_name] = stopwords\n",
    "\n",
    "# Now positive_and_negative_words_dict will contain words extracted from each text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name, stopwords in positive_and_negative_words_dict.items():\n",
    "    stopwords_dict[file_name] = {word.lower() for word in stopwords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the nltk tokenizer if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to calculate Positive Score\n",
    "def calculate_positive_score(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    positive_words = positive_and_negative_words_dict.get('positive-words.txt', set())\n",
    "    positive_score = sum(1 for token in tokens if token.lower() in positive_words)\n",
    "    return positive_score\n",
    "\n",
    "# Function to calculate Negative Score\n",
    "def calculate_negative_score(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    negative_words = positive_and_negative_words_dict.get('negative-words.txt', set())\n",
    "    negative_score = sum(1 for token in tokens if token.lower() in negative_words)\n",
    "    return negative_score   \n",
    "\n",
    "# Function to calculate Polarity Score\n",
    "def calculate_polarity_score(positive_score, negative_score):\n",
    "    denominator = positive_score + negative_score + 0.000001  # Add a small value to avoid division by zero\n",
    "    polarity_score = (positive_score - negative_score) / denominator\n",
    "    return polarity_score\n",
    "\n",
    "# Function to calculate Subjectivity Score\n",
    "def calculate_subjectivity_score(positive_score, negative_score, total_words):\n",
    "    denominator = total_words + 0.000001  # Add a small value to avoid division by zero\n",
    "    subjectivity_score = (positive_score + negative_score) / denominator\n",
    "    return subjectivity_score\n",
    "\n",
    "# Iterate over each row in the dataframe and calculate variables\n",
    "for index, row in extracted_data_df.iterrows():\n",
    "    text = row['Text']\n",
    "    total_words = len(nltk.word_tokenize(text))\n",
    "    positive_score = calculate_positive_score(text)\n",
    "    negative_score = calculate_negative_score(text)\n",
    "    polarity_score = calculate_polarity_score(positive_score, negative_score)\n",
    "    subjectivity_score = calculate_subjectivity_score(positive_score, negative_score, total_words)\n",
    "    \n",
    "    # Update the dataframe with the calculated values\n",
    "    extracted_data_df.at[index, 'Positive Score'] = positive_score\n",
    "    extracted_data_df.at[index, 'Negative Score'] = negative_score\n",
    "    extracted_data_df.at[index, 'Polarity Score'] = polarity_score\n",
    "    extracted_data_df.at[index, 'Subjectivity Score'] = subjectivity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to calculate Average Sentence Length\n",
    "def average_sentence_length(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    return len(words) / len(sentences)\n",
    "\n",
    "# Function to calculate Percentage of Complex Words\n",
    "def percentage_complex_words(text, complex_words):\n",
    "    words = word_tokenize(text)\n",
    "    complex_word_count = sum(1 for word in words if word in complex_words)\n",
    "    return complex_word_count / len(words)\n",
    "\n",
    "# Define a set of complex words\n",
    "# You can use a list of complex words obtained from a dictionary or any other reliable source\n",
    "complex_words = set(['complex', 'difficult', 'advanced', 'technical', 'sophisticated'])  # Example set of complex words\n",
    "\n",
    "# Function to calculate Fog Index\n",
    "def fog_index(avg_sentence_length, percentage_complex_words):\n",
    "    return 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "# Apply the functions to the DataFrame\n",
    "extracted_data_df['Average Sentence Length'] = extracted_data_df['Text'].apply(average_sentence_length)\n",
    "extracted_data_df['Percentage of Complex Words'] = extracted_data_df['Text'].apply(lambda x: percentage_complex_words(x, complex_words))\n",
    "extracted_data_df['Fog Index'] = fog_index(extracted_data_df['Average Sentence Length'], extracted_data_df['Percentage of Complex Words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from textstat import textstat\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a set of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to calculate Average Sentence Length, Percentage of Complex Words, Fog Index, Word Count, Syllable Count Per Word, Personal Pronouns Count, Average Word Length\n",
    "def calculate_metrics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    words_lower = [word.lower() for word in words]\n",
    "    words_no_stopwords = [word for word in words_lower if word not in stop_words and word not in string.punctuation]\n",
    "    complex_words = [word for word in words_no_stopwords if textstat.syllable_count(word) > 2]\n",
    "    personal_pronouns = len(re.findall(r'\\b(?:I|we|my|ours|us)\\b', text, flags=re.IGNORECASE))\n",
    "\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    percentage_complex_words = len(complex_words) / len(words)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    word_count = len(words_no_stopwords)\n",
    "    syllable_count_per_word = sum(textstat.syllable_count(word) for word in words) / len(words)\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "\n",
    "    return avg_sentence_length, percentage_complex_words, fog_index, word_count, syllable_count_per_word, personal_pronouns, avg_word_length\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "extracted_data_df[['Average Sentence Length', 'Percentage of Complex Words', 'Fog Index', 'Word Count', 'Syllable Count Per Word', 'Personal Pronouns', 'Average Word Length']] = extracted_data_df['Text'].apply(calculate_metrics).apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Positive Score</th>\n",
       "      <th>Negative Score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>Subjectivity Score</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex Words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Average Number of Words Per Sentence</th>\n",
       "      <th>Complex Word Count</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Syllable Count Per Word</th>\n",
       "      <th>Personal Pronouns</th>\n",
       "      <th>Average Word Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>rising cities impact economy, environment, inf...</td>\n",
       "      <td>huge development dependence people technology ...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.049459</td>\n",
       "      <td>10.109375</td>\n",
       "      <td>0.194745</td>\n",
       "      <td>4.121648</td>\n",
       "      <td>10.109375</td>\n",
       "      <td>147</td>\n",
       "      <td>507.0</td>\n",
       "      <td>1.587326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.489954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                              Title  \\\n",
       "0  blackassign0001  rising cities impact economy, environment, inf...   \n",
       "\n",
       "                                                Text  Positive Score  \\\n",
       "0  huge development dependence people technology ...            26.0   \n",
       "\n",
       "   Negative Score  Polarity Score  Subjectivity Score  \\\n",
       "0             6.0           0.625            0.049459   \n",
       "\n",
       "   Average Sentence Length  Percentage of Complex Words  Fog Index  \\\n",
       "0                10.109375                     0.194745   4.121648   \n",
       "\n",
       "   Average Number of Words Per Sentence  Complex Word Count  Word Count  \\\n",
       "0                             10.109375                 147       507.0   \n",
       "\n",
       "   Syllable Count Per Word  Personal Pronouns  Average Word Length  \n",
       "0                 1.587326                1.0             5.489954  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data_df.insert(1, 'URL', url_df['URL'])\n",
    "extracted_data_df.drop(columns=['Text', 'Title'], inplace=True)\n",
    "extracted_data_df.to_excel('Output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
